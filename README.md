# Stock-Analysis-using-RNN-LSTM

## Overview 

The project outlined focuses on predicting the performance of the Nifty 50 index for the fiscal year 2022/23 using various statistical approaches, deep learning techniques like Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) models, and additional methodologies to enhance predictive accuracy. The process involves several steps, including data collection, pre-processing, model building, feature engineering, training, testing, and evaluation of the predictive models. The overall framework includes:

## Data Collection and Pre-processing:
Collecting stock market information for Nifty 50 companies, including prices, trading volumes, and relevant metrics.

Data cleaning to rectify errors and handle missing values.

Scaling and normalization of data for uniformity and easier model training.

## Feature Engineering and Selection:
Deriving additional indicators (e.g., returns, log returns, percentage changes) to capture price swings and trends.

Incorporating technical indicators and other external data sources to enhance model predictive ability.

## Model Development using RNN and LSTM:
Utilizing TensorFlow in Python to construct RNN and LSTM models.

Designing multi-layered LSTM cells to capture temporal relationships in sequence data.

Implementing components like dropout layers and batch normalization to prevent overfitting.

## Model Training and Testing:
Splitting the dataset into training (80%) and testing (20%) sets for model evaluation.

Adjusting hyperparameters (learning rate, batch size, epochs) for optimal model performance.

Assessing model accuracy, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) for evaluation.

## Enhancement Techniques:
Employing indicators such as Relative Strength Index (RSI), Exponential Moving Average Forecast (EMAF), and Exponential Moving Average (EMA) to validate predictions.

Conducting secondary data analysis for informed insights into Nifty 50 index performance.

## Limitations and Ethical Considerations:
Acknowledging limitations such as index composition and its correlation with the broader economy.

Addressing ethical considerations like data privacy, bias reduction, market manipulation prevention, and aiming for the study's positive impact on economic growth and financial ecosystem health.

Overall, this project aims to provide a comprehensive and systematic approach to forecasting the Nifty 50 index's performance for the fiscal year, integrating various data sources, advanced modeling techniques, and ethical considerations to ensure responsible research practices.


### Moving-Average Data Preparation with Target Variables

Data Frame data is manipulated for analysis or modelling. First, a new column entitled 'Target Next Close' is established to indicate the modified closing price for the following trading day. This allows forecasting or prediction based on the current day's data and the next day's closing price. Dropna() removes missing rows from the Data Frame. 'Volume', 'Close', and 'Date' are removed from the Data Frame, and the index is reset to align data. Next, a 'Target' column shows the difference between each day's Adjusted Close and Open prices. The 'Target' column is moved one day back using shift (-1), aligning it with the previous day's data and making it appropriate for target variable usage in price prediction. These stages prepare data for analysis, modelling, or investing insights. All of the dataset's 'Close' prices are shown against three distinct EMAs (the shortest being 20 days, the longest being 100 days). EMAs are a specific kind of moving average that gives more weight to more current data while downplaying the significance of older data. In most cases, the first 'Close' price in the data set is used as the starting point for the EMA computation. The EMA is then calculated on a daily basis by averaging the 'Close' price and the EMA from the previous day. The current 'Close' price is given more or less importance depending on the period selected (20 days, 100 days, or 150 days). Longer time periods smooth out the EMA and bring attention to longer-term patterns, while shorter periods make the EMA more sensitive to rapid price changes. The resultant EMAs, EMAF, EMAM, and EMAS, help traders and analysts spot probable buy and sell signals, trend reversals, and market direction by providing useful insights into short-, medium-, and long-term price patterns, respectively.  The script generates variables for future research or trading strategies using the daily spread between the Adjusted Close and Open. The market started the day lower than it closed the day before, which accounts for the disparity. To make forecasts based on past data, the goal values are pushed backwards by one day to match the related price data. The target variable is used to construct a binary target class, in this case labelling price changes as either positive or negative. When the goal value is greater than zero, the target class is given the value 1. This indicates a positive price movement from open to closed. When the goal value is negative, however, the corresponding target class is 0. The revised closing price for the next trading day, preparing the data for further study.

### Exploring the Impact of Epochs on Model Training

The results are shown in Figure 4.2 that there is a strong interaction between the number of epochs used and the precision of forecasts. The model has a tendency to underfit when there are 30 epochs than desired, which results in poor predicting performance. The accuracy of the model will continue to improve with an increased number of epochs; however, there is a limit of decreasing returns beyond which there is a risk of the model being overfitting. The validation subset contributes to the determination of the ideal epoch count by helping to strike a balance between convergence and generalisation. Additionally, the selection of back candles affects the accuracy of the forecast. Short-term patterns can be captured by a lower number of back candles, while longer-term trends may be included by using a higher number of back candles. It has been noticed that providing an accurate portrayal of the historical background in a balanced manner leads to better accuracy. The capacity of the model to capture both short-term and long-term trends is enhanced when an ideal number of epochs and back candles are used in conjunction with one another. 

### Split Training and Testing with 60 Epochs, 30 Batches, 30 Back Candles

Results from experiments using an epoch count of 60 and a batch size of 30 showed unsatisfactory performance, especially when applied to the prediction of the Nifty 50 index. This exemplifies the issues associated with overfitting, when a model grows too complicated and fits noise in the training data, ultimately leading to subpar results on novel data. High epoch count (training iterations) and low batch size were shown to cause this overfitting. The discrepancy between the training and testing dataset results pointed to overfitting, highlighting the need for cautious hyperparameter selection and model evaluation if accurate forecasts are to be made, particularly in intricate financial markets like the Nifty 50. This disparity highlights the dangers of employing high epoch counts and small batch sizes, especially when attempting to anticipate complicated financial markets like the Nifty 50 index. These results highlight the need for careful hyperparameter selection and thorough model assessment for making reliable predictions that extend beyond the training data.

### Comparative Analysis of Testing and Training Data Sets: Model Performance Evaluation: 

The extensive testing and training methods that were carried out as part of this investigation have resulted in incredibly promising findings, which have shown an astounding degree of similarity between the two datasets. The outstanding predictive power of trained Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) models has resulted in predictions that impressively correlate with observations made in the actual world. The durability of these models in identifying detailed patterns within the data is shown by the accuracy levels that have been attained, which are often over 90 per cent. This remarkable accuracy not only demonstrates the strength of RNN and LSTM architectures but also attests to the efficiency of the hyperparameters, input characteristics, and training approach that were used. The absence of overfitting is shown by the fact that the models can successfully generalise to data they have not before seen when the outcomes of training and testing are in perfect harmony with one another. This kind of congruence also highlights the practical relevance of these models in situations that take place in the real world, when making correct forecasts is of the utmost significance. This model was the end result of the combination. Because of this, the results throw up the door for enhanced decision-making in the financial markets, guided by forecasts that are not only trustworthy but also carry a significant amount of predictive potential. 

It is impossible to overestimate the significance of the role that data visualisation played in validating the results of this investigation. The line graph that graphically illustrates the convergence of forecasts after combining the information from the 60 back candles, Epochs, and batch size is a compelling representation of the success that was obtained and is a remarkable indication of the success that was achieved. The visual portrayal provides a concrete sense of the model's capacity to recognise subtle trends, patterns, and relationships within the data, which ultimately results in predictions that reflect real-world observations with an impressive level of accuracy. The story of the research being able to achieve near alignment between training data and testing data is supported by the constant convergence trend shown by the line graph. This graphical depiction also functions as a bridge between quantitative analysis and intuitive perception, making the conclusions accessible to a larger audience

### Adjustment to New Models and Complexity

In the context of machine learning training, the experimental inquiry that was carried out in this work had the goal of illuminating the complicated link that exists between the number of epochs, the processing time, and the model performance. We wanted to have a better understanding of the complex dynamics that lay behind this basic training parameter, so we systematically changed the number of epochs, which is shorthand for the number of times the training dataset was traversed in its entirety, and we did so using three different settings: 30, 60, and 100 epochs. The results of our investigation revealed a clear trade-off between the accuracy of the model and the processing time. A visible improvement in the accuracy of the model was seen as a result of an increase in the number of epochs, and this improvement took the form of a gradual decrease in the values of loss. This empirical confirmation of the iterative nature of learning provides additional substantiation of the model's potential to grasp underlying patterns within the data. On the other hand, this increase in accuracy was accompanied by a corresponding increase in overall processing time. This occurred because the addition of additional epochs resulted in longer training periods. It is important to keep in mind that the lengthening of the processing time, is an indication of the model's increasing complexity and has practical ramifications for applications that are time-sensitive, as one should do so with caution. In addition, while the convergence of loss values indicates a better alignment between predictions and the ground truth, careful attention must be given to the possibility of overfitting. This refers to the situation in which a model may perform very well on training data but struggle when presented with data it has not previously encountered. In light of these results, it is necessary to make strategic decisions to achieve an ideal balance between the refinement of the model and the efficiency with which it processes data. These choices might be influenced by methods such as early halting or dynamic learning rate adaption. The findings of this work add to a more thorough knowledge of the relationship between training dynamics and model effectiveness, which advances our capacity to design capable machine learning models within the limitations of the actual world.

### Inadequate EMA’s:

Additionally, the poor performance that is connected with the use of a limited number of Exponential Moving Averages (EMAs) might be due to the inherent restrictions that come along with having a restricted scope. Because there were fewer EMAs in play, the model had a difficult time taking into account the full scope of previous price patterns and swings. This narrow viewpoint failed to effectively convey the delicate interaction between short-term and long-term market dynamics, which resulted in an inadequate portrayal of price patterns. Consequently, this description of pricing patterns was inaccurate. As a consequence of this, the forecasts that were obtained from this limited perspective were unable to capture the whole range of conceivable outcomes, which resulted in a discernible disparity between the projections and the actual behaviours of the market. This highlights the significant role that a varied variety of EMAs play in delivering a more complete and accurate representation of price fluctuations, enabling a thorough knowledge of market patterns, and allowing for the refining of prediction models. 


As I progressed through this research, I became more familiar with the world of exponential moving averages (EMAs), and at that time, I made an amazing discovery. Notably, while doing experiments with a far lower total number of EMAs, the difficulty of incorporating these indications into the model became readily apparent.  This position was made much more difficult by the discovery that the ensuing forecasts were not even close to being accurate when compared to the actual results of the market. The disparity between the anticipated and actual values highlighted how essential it is to have access to a wider variety of EMAs to accurately represent the complex relationships that are inherent in financial data. This illuminating discovery shows the important role that accurate data points and indicators play in the process of developing prediction models and emphasises the need for a balanced and comprehensive analytical framework.

### EMA’s Analysis: 

Within the confines of this project, I've set out on an in-depth investigation to investigate the dynamic interplay between varying window sizes and parameter settings in relation to significant financial indicators. These include the Relative Strength Index (RSI) and three different Exponential Moving Averages (EMAs) EMAF, EMAM, and EMAS. This endeavour is intended to be as exhaustive as possible. The results of this investigation have produced a variety of insights into the complex mechanisms behind market behaviour and the interpretation of trends.

A significant change in the interpretative capacity of these indicators was discovered to take place as soon as the window lengths were cut down. In particular, the shift from longer to shorter periods increased the sensitivity of the relative strength index (RSI) to quick price fluctuations. As a result, the RSI generated more frequent alerts for overbought and oversold circumstances. Because of this increased sensitivity, short-term trends and acute market dynamics were brought into sharper focus. The impact was replicated in EMAs, as the decrease in window size boosted their response to quick price fluctuations, catching transitory trends that could otherwise be obscured by noise. This effect was mirrored in EMAs. This realignment towards a micro-level study proved to be quite helpful in detecting the quick and fleeting price swings that were occurring.

The examination of EMAS, EMAM, and EMAF, where the highest frequencies of 50, 100, and 150 were used, respectively, was where the majority of the focus of the study was placed. An intriguing phenomenon began to take shape as I delved further into my research and conducted it for longer periods. Notably, the forecasts achieved a higher level of accuracy after using these longest durations for their respective insights. It was clear that synchronising these indicators with the real behaviour of the market within the longest analytical periods made it possible to generate astonishingly accurate forecasts. This noteworthy relationship between expanded analytical viewpoints and predictive power highlights the potential significance of long-term insights in the context of boosting the trustworthiness of market projections.

Because of this endeavour, I now have a deep and nuanced comprehension of the complex processes that drive different window widths and parameter combinations in financial indicator analysis. The process of fine-tuning these characteristics, which may range from using shorter windows for increased sensitivity to making effective use of the highest EMA frequencies, has been an important factor in improving the accuracy of market forecasts. This study highlights the balance between capturing transient market fluctuations and uncovering persistent patterns, which enables me to develop a well-informed analytical methodology that corresponds perfectly with the particular aims of this investigation.

### Increasing the parameters for higher accuracy 

The modifications that were made to the parameters, which included increasing the number of epochs to 100, increasing the batch size to 50, and extending the input back candles to 60 days, had positive results in terms of the accuracy of the predictions. These modifications were implemented into the model without causing any disruptions, which resulted in predictions that showed a remarkable convergence across the datasets used for training and testing. Together, the thorough engineering of these features and the precise orchestration of these factors were essential in creating predictions that nearly resembled the behaviour that occurred in the real world. This accomplishment highlights how important it is to maintain a healthy interaction between parameter tweaking and feature engineering to get optimal results. The capability of the model to catch nuanced patterns within the data as well as its enhanced prediction performance are two examples of the productive synergy that can be achieved when technology refinement is combined with domain-specific insights. As a consequence of this, the recent developments offer a great deal of promise in terms of enhancing the dependability and effectiveness of forecasts about the financial market, with possible applications that transcend beyond the scope of this particular dataset.

## Future Research Recommendations 

The concrete findings obtained from the interaction between the Nifty 50 index and Gross Domestic Product (GDP) establish the framework for future research endeavours that might result in a greater knowledge of the complex link that exists between financial markets and the performance of the economy. The environment of finance and economics is always shifting, and the ideas for future study that are provided below provide potential new directions for investigation.

### Dynamic Modelling and comparison on a global scale:  

Conduct research on the various dynamic modelling methodologies that may represent the changing effect of the Nifty 50 index on GDP during the course of a variety of economic cycles. This may entail the use of time-series analysis, algorithms for machine learning, or hybrid models that are able to adjust to the ever-changing market circumstances. Carry out research that spans many nations and aims to analyse the relationship between national stock indexes and GDP in a variety of different economies. This comparative research endeavour is set to uncover intricate and unique links as a result of a complete investigation of various market systems, economic policies, and external variables. These kinds of insights may shed light on the complex processes that define the influence that stock indexes have on economic performance, providing a greater knowledge of how local issues interact with global trends in the process.

### Utilise Exogenous Information and Models That Can Be Read: 

Consider adding other Data While the primary emphasis of your study is on stock market data and economic indicators, you should also consider adding other data sources that have the potential to affect stock prices. Some examples of such data sources are news sentiment analysis, international market movements, and geopolitical events. This may result in more accurate forecasts as well as a deeper grasp of the dynamics of the market. Models That Can Be Interpreted Even while complicated models like RNN and LSTM are capable of making correct predictions, these models may not always be able to be interpreted. Exploring approaches to make the models more interpretable, such as feature importance visualisation or SHAP (Shapley Additive exPlanations) analysis, may reveal further insights into the components that are driving predictions if they are investigated.
